[
  {
    "objectID": "Teaching.html",
    "href": "Teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Introduction to Social Psychology\nPersonality Psychology\nPsychology of Women"
  },
  {
    "objectID": "Teaching.html#psychology",
    "href": "Teaching.html#psychology",
    "title": "Teaching",
    "section": "",
    "text": "Introduction to Social Psychology\nPersonality Psychology\nPsychology of Women"
  },
  {
    "objectID": "Teaching.html#political-science",
    "href": "Teaching.html#political-science",
    "title": "Teaching",
    "section": "Political Science",
    "text": "Political Science\n\nIntroduction to American Politics\nRace, Innocence, and the Death Penalty"
  },
  {
    "objectID": "Teaching.html#methods",
    "href": "Teaching.html#methods",
    "title": "Teaching",
    "section": "Methods",
    "text": "Methods\n\nUndergraduate Introduction to Statistics and R Programming (Psychology and Political Science)"
  },
  {
    "objectID": "Research.html",
    "href": "Research.html",
    "title": "Research",
    "section": "",
    "text": "Belief Systems, Ideology, Attitude Moralization, Individual Differences, Political Polarization, Survey Research, Observational Causal Inference, Computational Social Science"
  },
  {
    "objectID": "Research.html#my-interests",
    "href": "Research.html#my-interests",
    "title": "Research",
    "section": "",
    "text": "Belief Systems, Ideology, Attitude Moralization, Individual Differences, Political Polarization, Survey Research, Observational Causal Inference, Computational Social Science"
  },
  {
    "objectID": "Research.html#publications",
    "href": "Research.html#publications",
    "title": "Research",
    "section": "Publications",
    "text": "Publications\nCassario, A.L., Vallabha, S., Thompson, J.L., Carrillo, A., Solanki, P., Gnall, S.A., Rice, S., Wetherell, G., & Brandt, M.J. (2024) Registered report: Cognitive ability but not cognitive reflection predicts political animosity and favouritism. British journal of social psychology, (in press) Paper Supplemental Materials\nCassario, A.L., & Brandt, M.J. (2024). Testing theories of threat, individual difference, and ideology: Little evidence of personality based individual differences in ideological responses to threat. Social psychological and personality science, (in press) Paper Supplemental Materials\nCassario, A.L. (2023). Perceived vulnerability to infectious disease and percieved harmfulness are as predictive of citizen response to COVID-19 as partisanship. Politics and the life sciences, 42(2), 277-290. Paper Supplemental Materials\nBrandt, M.J., & Cassario, A.L. (2023) Distinguishing Between Worldview Conflict and Shared Alliances: Commentary on Pinsof, Sears, and Haselton. Psychological inquiry, 34 (3), 168-174. Paper\nCox, K. S., Hanek, K. J., & Cassario, A. L. (2019). Redemption in a single low point story longitu- dinally predicts well-being: The incremental validity of life story elements. Journal of personality, 87(5), 1009-1024."
  },
  {
    "objectID": "Research.html#accepted-stage-1-registered-reports",
    "href": "Research.html#accepted-stage-1-registered-reports",
    "title": "Research",
    "section": "Accepted Stage 1 Registered Reports",
    "text": "Accepted Stage 1 Registered Reports\nWetherell, G., Thompson J.T., Brandt, M.J., Cassario, A.L., Rice, S., Solanki, P., Carrillo, A. & Vallabha S. Do mismatches between individual and target group personality predict prejudice? Collabra: Psychology.\nThompson, J.L., Cassario, A.L., Rice, S., Gnall, S.A., Carrillo, A., Solanki, P., Vallabha S., Brandt, M.J., & Wetherell G., Registered report: stress testing predictive models of ideological prejudice. PLOS one."
  },
  {
    "objectID": "Research.html#revise-and-resubmit",
    "href": "Research.html#revise-and-resubmit",
    "title": "Research",
    "section": "Revise and Resubmit",
    "text": "Revise and Resubmit\nRice, S., Cassario A.L., Thompson J.L., Gnall S.A., Carrillo, A., Vallabha S., Solanki P., Brandt M.J., Wetherell G., Registered report: Exploring the relationship between nationalism, patriotism, and prejudice. Comprehensive results in social psychology."
  },
  {
    "objectID": "Research.html#under-review",
    "href": "Research.html#under-review",
    "title": "Research",
    "section": "Under Review",
    "text": "Under Review\nCassario, A.L., & Brandt, M.J. Testing Predictions About Individual Level Belief System Structure Using the Conceptual Similarity Task.Paper Supplemental Materials"
  },
  {
    "objectID": "Research.html#working-projects",
    "href": "Research.html#working-projects",
    "title": "Research",
    "section": "Working Projects",
    "text": "Working Projects\nCassario, A.L., & Brandt, M.J., Does personality moderate the effect of threat on political conservatism? Experimental evidence says no.\nCassario, A.L., & Brandt, M.J., Exploring the effects of downward mobility on attitudes, behaviors, and life outcomes.\nCassario, A.L., Mulwa, K., & Brandt, M.J., Does a competitive presidential campaign increase interest in, desire to consume, and desire to share misinformation?\nCassario, A.L., Mulwa, K., & Brandt, M.J., Asymmetrical perceptions? Do Co and Out-partisans view Republicans as more likely to believe and be interested in misinformation and does this drive greater sharing of misinformation on the right?"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Abigail Cassario: PhD Student in Psychology",
    "section": "",
    "text": "Welcome to my website! I am a third-year PhD Student in the Department of Psychology at Michigan State University and a member of the Belief Systems Lab. My research and teaching interests lie in social and political psychology and research methods. Specifically, I am interested in how socialization and individual differences shape people’s belief systems, and relatedly how characteristics of people’s belief systems affect their individual attitudes (e.g., moral conviction).\nI also enjoy learning about computational social science. I am particularly interested in improving causal inferences in psychology, and in machine learning methods (e.g., random forests, tree-boosting, causal forests).\nI was a division one swimmer as an undergraduate at the University of North Carolina at Asheville (go Bulldogs!), and still enjoy swimming and weightlifting in my free time. I am also a huge sports fan, and enjoy cheering on my hometown Carolina Hurricanes with my dad and little brother during hockey season!\nBefore MSU, I received my BA in political science and psychology from the University of North Carolina at Asheville, and my MA in political science from the University of North Carolina at Chapel Hill."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "About this site\n\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog",
    "section": "",
    "text": "A Practical Introduction to Tree-Based Machine Learning Methods\n\n\nA beginner friendly introduction to supervised, tree-based methods through simulation\n\n\n\nAbigail Cassario\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2024-10-03/index.html",
    "href": "posts/2024-10-03/index.html",
    "title": "A Practical Introduction to Tree-Based Machine Learning Methods",
    "section": "",
    "text": "Post in progress"
  },
  {
    "objectID": "posts/2024-10-03/index.html#intro",
    "href": "posts/2024-10-03/index.html#intro",
    "title": "A Practical Introduction to Tree-Based Machine Learning Methods",
    "section": "",
    "text": "As social scientists, and as humans, we are increasingly being confronted with a barrage of data. Platforms like Amazon, Netflix, and Spotify collect and use vast amounts of data on users’ preferences, behaviors, and interactions to target consumers with personalized recommendations. As consumers we have access to our own in depth health and fitness data stored by our smart-watches and accompanying smart phone applications, Spotify sends us our music streaming data year after year, and our medical data is available at the click of a button via websites and applications from healthcare providers. Collecting data from research participants is also now easier, quicker, and cheaper than ever before thanks to platforms like Qualtrics, Prolific, and Forthright. At the same time this ease of data collection has also led to wider datasets with (at least some subset) of research participants providing information on hundreds of variables."
  },
  {
    "objectID": "posts/2024-10-03/index.html#those-pesky-assumptions-of-the-linear-model",
    "href": "posts/2024-10-03/index.html#those-pesky-assumptions-of-the-linear-model",
    "title": "A Practical Introduction to Machine Learning in Psychology",
    "section": "Those pesky assumptions of the linear model…",
    "text": "Those pesky assumptions of the linear model…\nWhat’s a psychologist to do with this deluge of information? The possibilities are endless. We can create interventions targeted to specific individuals, predict who is most at risk of developing serious mental health conditions such as depression and bipolar disorder, and tailor interventions to particular subsets of the population vulnerable to certain inaccurate and potentially dangerous beliefs (like the belief that vaccines cause Autism).\nBut our traditional methods (ANOVA, linear regression) don’t handle these tasks or the structure of big data well. There are several reasons for this, but they all boil down to this type of data being likely to violate assumptions of the linear model. To name a few, traditional methods can’t handle high dimensional data (data where the number of predictors exceeds the number of observations). In linear regression, when the number of predictors outnumbers observations over-fitting occurs and our estimates become unreliable. Likewise, and this is particularly relevant for psychological scientists, linear regression and ANOVA make often unrealistic assumptions about our ultimately unknown, true data-generating process (DGP). Specifically, these methods assume that the relationship between our variables are linear (or additive) and that residuals are normally distributed. For example linear regression assumes our DGP takes the form of:\n\\[\nY_i = B_0 + B_1X_{1i} + B_2X_{2i} + B_kX_{ki} + \\epsilon_i\n\\] But (as mentioned by Simmons in this paper) if our true DGP is non-linear, and highly complex, such as:\n\\[\nY_i = \\frac{3000*X_{1i}}{\\sqrt[3]{X_{2i}}} + \\epsilon_i\n\\] Linear regression would be unable to model such a relationship, or accurately predict our \\(Y_i's\\) using \\(X_{1i}\\) and \\(X_{2i}\\).\nEnter tree based methods."
  },
  {
    "objectID": "posts/2024-10-03/index.html#assumptions-about-the-data-generating-process-dgp",
    "href": "posts/2024-10-03/index.html#assumptions-about-the-data-generating-process-dgp",
    "title": "A Practical Introduction to Tree-Based Machine Learning Methods",
    "section": "Assumptions about the data generating process (DGP)",
    "text": "Assumptions about the data generating process (DGP)\nWhat’s a psychologist to do with this deluge of information? The possibilities are endless. We can create interventions targeted to specific individuals, predict who is most at risk of developing serious mental health conditions such as depression and bipolar disorder, and tailor interventions to particular subsets of the population vulnerable to certain inaccurate and potentially dangerous beliefs (like the belief that vaccines cause Autism).\nBut our traditional methods in psychology (ANOVA, linear regression) don’t handle these tasks or the structure of big data well. There are several reasons for this, but they all boil down to this type of data being likely to violate assumptions of the data generating process (DGP) of the linear model. There are five assumptions that this entails, but for the purposes of this blog post we’ll focus on violation of the linearity assumption. To assume linearity means that we assume a linear, additive relationship between our predictors included in \\(X\\) and our \\(Y\\) outcome, and that there are no non-linear relationships between our predictors and our outcome or higher order interactions unless they are explicitly included in the model.\nIn mathematical terms, we can say that the linear model implies that our DGP takes the following form:\n\\[\nY_i = B_0 + B_1X_{1i} + B_2X_{2i} + B_kX_{ki} + \\epsilon_i\n\\]\nBut frequently, our data generating processes are much more complex. Consider the following toy example with two predictor variables, \\(X_1\\) and \\(X_2\\) imagine that:\n\\[\nY_i = (\\frac{3000*X_{1i}}{\\sqrt[3]{X_{2i}}})*025 + \\epsilon_i\n\\] Linear regression would be unable to detect such a relationship, or accurately predict our \\(Y_i's\\) using \\(X_{1i}\\) and \\(X_{2i}\\), even though the relationship between our \\(X\\) predictors and \\(Y\\) is systematic, and \\(X_1\\) and \\(X_2\\) should be highly informative in predicting \\(Y\\).\nThis is a problem for us as psychologists. The complex phenomeona we study in all likelihood frequently fail to meet the linearity assumption implied by equation 1 above."
  },
  {
    "objectID": "posts/2024-10-03/index.html#tree-based-methods",
    "href": "posts/2024-10-03/index.html#tree-based-methods",
    "title": "A Practical Introduction to Tree-Based Machine Learning Methods",
    "section": "Tree based methods",
    "text": "Tree based methods\nEnter tree based methods. Tree based methods are a family of models that are particularly well suited for making predictions and identifying systematic relationships in the data when the true data generating process is highly complex, containing non-linearities and deep interactions (Montgomery & Olivella, 2018). This is a situation that we’re frequently confronted with when dealing with big data. Instead of assuming a linear, additive DGP, tree based methods are flexible and data driven, so we don’t have to begin our analysis with strict (often unwarranted) theoretical assumptions that our true DGP is additive and linear.\n\nClassification and regression trees (CART)\nDifferent tree based methods vary in their particular algorithms, but they are united by the fact that they all rely on regression and classification trees. Regression and classification trees work by dividing the data into smaller subsets based on covariates that are useful in predicting the outcome. The idea is to split the data into groups with similar values of the outcome. For example, if we want to predict an outcome, like frequency of posting to social media, and we have a number of predictors (e.g., age, extroversion, neuroticism, technology literacy, need for chaos, field of employment, employment status, region of residence, educational attainment), the tree looks at our predictors, determines which are useful in predicting the outcome, and groups similar observations together. The average outcome in each group becomes our prediction for new data points that fall into that group based on their values of our relevant predictors.\nIn more formal terms, regression and classification trees generate predictions by repeatedly splitting the data based on covariate values. These splits create what we call branches. For example, we may start with a complete dataset, then if predictor \\(X_1\\) is useful in predicting \\(Y_i\\), we may split the data such that observations with \\(X_1\\) &lt; 5 constitute one subset, and observations with \\(X_1\\) &gt;= 5 constitute another subset. Then, we’d continue to look for relevant predictors to once again split each of our subsets on. So, for instance, if for our \\(X_1\\) &lt; 5 group, observations with \\(X_2\\) &gt;0 tend to have similar values of \\(Y_i\\) and our \\(X_2\\) &lt;= 0 observations tend to have similar values of \\(Y_i\\) we’d create another branch splitting our data again. If there were no more relevant predictors to split on, our \\(X_2\\) split would be our final split for that subset of the data, and we would generate a predicted \\(Y_i\\) by taking the average \\(Y_i\\) value of data points falling into that subset. This predicted \\(Y_i\\) at the end of our final brach(es) is called a “leaf.” This process is shown visually in the figure below.\n\n\n\nAn ugly doodle of what the regression tree described above could look like. The internal splits are called branches, the predictions at the end of the branches are called leafs.\n\n\nThe verbal description of this process may still seem a bit confusing, so we’ll also talk through a formal mathematical representation drawn from a paper by Montgomery & Olivella (2018, p. 732, equation 1). We’ll be revisiting this paper a few times throughout the remainder of this blog post, since out of the papers I’ve read on this topic, these authors strike the best balance between presenting enough math readers gain an understanding of what these models are doing under the hood, while keeping things relatively accessible to an applied audience. Equation 1 from this paper is shown below.\n\\[\nf({Xi}) = T(X_{i}; \\Theta) \\equiv c_{b}I(X_{i} \\in R_{b})\n\\] In layman’s terms, the equation indicates that the prediction region for an observation with observed covariates \\(X_i\\) is equal to a decision tree defined by the set of predictors \\(X_i\\) and model hyper-parameters (e.g., tree depth, node size) \\(\\Theta\\), which is equivalent to the sum of the \\(c_b\\) predictions for regions \\(b=1\\) to \\(B\\) which is multiplied by 1 if \\(X_i\\) falls into the region in question, \\(R_b\\), 0 otherwise before the summation. For continuous outcomes \\(c_b\\) is defined as \\(\\bar{y}\\) for all observations falling in that region.\nThus, choosing \\(\\Theta\\) optimally is important. In choosing \\(\\Theta\\) we seek to minimize the loss function, which in regression trees is usually the sum of squared errors (\\(y_i\\) - \\(c_b\\))\\(^2\\). CART uses recursive binary splitting to find the best combinations of predicted values and partitions. (This is just the process we described above, where the algorithm repeatedly splits the data into two groups that vary in the outcome of interest, until it can no longer split the data anymore.)\nHowever, this can result in repeatedly splitting the data until each unique combination of covariates becomes its own predictive region resulting in overfitting to noise and as a result poor predictive performance when the model sees new data. To prevent this, trees are pruned according to a penalty term \\(\\alpha B\\) where \\(B\\) is the number of unique regions. Even with the inclusion of \\(\\alpha B\\) single tree models can be vulnerable to arbitrary differences in splitting rules resulting in wildly different predictions across different single tree models fitted on the same data, and an inability to detect additive relationships when they are included in the true DGP (see Montgomery & Olivella, 2018, pg. 733 for a discussion). We’ll return to these shortcomings below, when we implement these methods using simple simulations.\nFortunately, ensemble methods, which combine many different single tree models to generate estimates retain the strengths of single tree models, while containing additional desirable properties that address their weaknesses.\n\n\nRandom forests\n\n\nGradient boosting machines\n\n\nBayesian additive regression trees (BART)"
  },
  {
    "objectID": "posts/2024-10-03/index.html#a-simple-simulation-linear-regression-breaks-down-when-our-assumed-dgp-doesnt-hold",
    "href": "posts/2024-10-03/index.html#a-simple-simulation-linear-regression-breaks-down-when-our-assumed-dgp-doesnt-hold",
    "title": "A Practical Introduction to Machine Learning in Psychology",
    "section": "A simple simulation: Linear regression breaks down when our assumed DGP doesn’t hold",
    "text": "A simple simulation: Linear regression breaks down when our assumed DGP doesn’t hold\nConsider the following simulation. We have two datasets consisting of two different data generating processes. One conforms to the assumed data-generating process of the linear model, the other takes the form of equation (2)."
  },
  {
    "objectID": "posts/2024-10-03/index.html#a-simple-simulation-what-happens-when-our-assumed-dgp-doesnt-hold",
    "href": "posts/2024-10-03/index.html#a-simple-simulation-what-happens-when-our-assumed-dgp-doesnt-hold",
    "title": "A Practical Introduction to Machine Learning in Psychology",
    "section": "A simple simulation: What happens when our assumed DGP doesn’t hold?",
    "text": "A simple simulation: What happens when our assumed DGP doesn’t hold?\nConsider the following simulation. We have two datasets consisting of two different data generating processes. One conforms to the assumed data-generating process of the linear model, the other takes the form of equation (2)."
  },
  {
    "objectID": "posts/2024-10-03/index.html#a-simple-simulation-what-happens-with-our-estimates-from-the-linear-model-when-our-assumed-dgp-doesnt-hold",
    "href": "posts/2024-10-03/index.html#a-simple-simulation-what-happens-with-our-estimates-from-the-linear-model-when-our-assumed-dgp-doesnt-hold",
    "title": "A Practical Introduction to Machine Learning in Psychology",
    "section": "A simple simulation: What happens with our estimates from the linear model when our assumed DGP doesn’t hold?",
    "text": "A simple simulation: What happens with our estimates from the linear model when our assumed DGP doesn’t hold?\nConsider the following simulation. We have two datasets consisting of two different data generating processes. One conforms to the assumed data-generating process of the linear model, the other takes the form of equation (2)."
  },
  {
    "objectID": "posts/2024-10-03/index.html#a-simple-simulation-what-happens-to-our-estimates-from-the-linear-model-when-our-assumed-dgp-doesnt-hold",
    "href": "posts/2024-10-03/index.html#a-simple-simulation-what-happens-to-our-estimates-from-the-linear-model-when-our-assumed-dgp-doesnt-hold",
    "title": "A Practical Introduction to Machine Learning in Psychology",
    "section": "A simple simulation: What happens to our estimates from the linear model when our assumed DGP doesn’t hold?",
    "text": "A simple simulation: What happens to our estimates from the linear model when our assumed DGP doesn’t hold?\nConsider the following simulation. We have two datasets consisting of two different data generating processes. One conforms to the assumed data-generating process of the linear model, the other takes the form of equation (2)."
  },
  {
    "objectID": "posts/2024-10-03/index.html#a-simple-simulation-what-happens-to-our-estimates-from-linear-regression-when-our-assumed-dgp-doesnt-hold",
    "href": "posts/2024-10-03/index.html#a-simple-simulation-what-happens-to-our-estimates-from-linear-regression-when-our-assumed-dgp-doesnt-hold",
    "title": "A Practical Introduction to Machine Learning in Psychology",
    "section": "A simple simulation: What happens to our estimates from linear regression when our assumed DGP doesn’t hold?",
    "text": "A simple simulation: What happens to our estimates from linear regression when our assumed DGP doesn’t hold?\nSo how does linear regression fair when the linearity assumption is violated. Not well, but you don’t have to just take my word for it. Consider the following simulation. We have two datasets consisting of two different data generating processes. One conforms to the assumed data-generating process of the linear model, the other takes the form of the second equation above. We’ll start with the DGP that conforms to the assumptions of linear regression, before seeing what happens to our predicted \\(Y_i\\)’s when our true DGP violates the linearity assumption.\n\n# First seet our seed since we're simulating with random draws\nset.seed(919)\n# Assumed DGP linear model \nX1 &lt;- rnorm(1000, mean = 10, sd = .5)\nX2 &lt;- runif(1000, min = 0, max = 10)\nY &lt;- .3*X1 + .2*X2 + .7*X1*X2 \nerror &lt;- rnorm(1000, mean=0, sd = .9)\nY &lt;- Y + error\n\n# combine into dataset \nlinear_sim &lt;- as.data.frame(cbind(Y, X1, X2)) \n\n# fit linear model \nmod &lt;- lm(Y ~ X1 + X2 + X1*X2, data = linear_sim)\n\nNow that we’ve simulated our data, and fitted our linear regression model, we can generate our predicted \\(\\hat{Y}_{i}\\) values using our model and store that information in our dataset. Then we can visually examine the accuracy of our model’s predictions by plotting the predicted \\(\\hat{Y_i}\\)’s against the observed \\(Y_i\\)’s. We can do this using like so:\n\nlinear_sim$y_hat &lt;- predict(mod)\n\n# plotting observed vs. fitted y's using ggplot \nlibrary(ggplot2)\nggplot(linear_sim, aes(x = Y, y = y_hat)) +\n  geom_point(alpha = 0.5) +  \n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +  \n  # add reference line,\n  #falling on the 45 degree line \n  # means observed and predicted match (perfect fit)\n  labs(x = \"Observed Y\", y = \"Predicted Y\", title = \"Observed vs Predicted Y\") +\n  theme_minimal()\n\n\n\n\nAs you can see, although there is a bit of variation (the random error in our data generating process), most observations fall very close to the 45 degree line. Our linear regression model does a very good job of predicting our Y.\nBut what if our true data generating process is highly non-linear and non additive? There’s a systematic relationship between \\(Y\\) and our predictor variables, but the relationship doesn’t take the form assumed by the linear model. Well, we can run a simulation and to see this too."
  },
  {
    "objectID": "posts/2024-10-03/index.html#a-simple-simulation-what-happens-to-our-estimates-from-linear-regression-when-linearity-doesnt-hold",
    "href": "posts/2024-10-03/index.html#a-simple-simulation-what-happens-to-our-estimates-from-linear-regression-when-linearity-doesnt-hold",
    "title": "A Practical Introduction to Tree-Based Machine Learning Methods",
    "section": "A simple simulation: What happens to our estimates from linear regression when linearity doesn’t hold?",
    "text": "A simple simulation: What happens to our estimates from linear regression when linearity doesn’t hold?\nSo how does linear regression fair when the linearity assumption is violated? Not well, but you don’t have to just take my word for it. Consider the following simulation. We have two datasets consisting of two different data generating processes. One conforms to the assumed data-generating process of the linear model, the other takes the form of the second equation above. We’ll start with the DGP that conforms to the assumptions of the linear model, before seeing what happens to our predicted vs. observed \\(Y_i\\)’s when our true DGP violates the linearity assumption.\n\n#------------------- Case 1: We meet the linearity assumption------- #\n# First seet our seed since we're simulating with random draws\nset.seed(919)\n# Assumed DGP linear model \nX1 &lt;- rnorm(1000, mean = 10, sd = .5)\nX2 &lt;- runif(1000, min = 0, max = 10)\nY &lt;- .3*X1 + .2*X2 + .7*X1*X2 \nerror &lt;- rnorm(1000, mean=0, sd = .9)\nY &lt;- Y + error\n\n# combine into dataset \nlinear_sim &lt;- as.data.frame(cbind(Y, X1, X2)) \n\n# fit linear model \nmod &lt;- lm(Y ~ X1 + X2 + X1*X2, data = linear_sim)\n\nNow that we’ve simulated our data, and fitted our linear regression model, we can generate our predicted \\(\\hat{Y}_{i}\\) values using our model and store that information in our dataset. Then we can visually examine the accuracy of our model’s predictions by plotting the predicted \\(\\hat{Y_i}\\)’s against the observed \\(Y_i\\)’s. We can do this using like so:\n\nlinear_sim$y_hat &lt;- predict(mod)\n\n# plotting observed vs. fitted y's using ggplot \nlibrary(ggplot2)\nplot &lt;- ggplot(linear_sim, aes(x = Y, y = y_hat)) +\n  geom_point(alpha = 0.5) +  \n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +  \n  # add reference line,\n  #falling on the 45 degree line \n  # means observed and predicted match (perfect fit)\n  labs(x = \"Observed Y\", y = \"Predicted Y\", title = \"Observed vs Predicted Y\") +\n  theme_minimal()\n\nAs you can see below, although there is a bit of variation (the random error in our data generating process), most observations fall very close to the 45 degree line. Our linear regression model does a very good job of predicting our \\(Y\\).\n\n\n\n\n\nBut what if our true data generating process is highly non-linear and non additive? There’s a systematic relationship between \\(Y\\) and our predictor variables, but the relationship doesn’t take the form assumed by the linear model. Let’s run another simulation and see.\n\n#------------------- Case 2: We FAIL to meet the linearity assumption------- #\n# First seet our seed since we're simulating with random draws\nset.seed(919)\n# Non-linear DGP\nX1 &lt;- rnorm(1000, mean = 10, sd = .5)\nX2 &lt;- runif(1000, min = 0, max = 10)\nY &lt;- (3000*X1)/((X2)^(1/3))*.025\nerror &lt;- rnorm(1000, mean=0, sd = .9)\nY &lt;- Y + error\n\n# combine into dataset \nnonlinear_sim &lt;- as.data.frame(cbind(Y, X1, X2)) \n\n# fit linear model \nmod2 &lt;- lm(Y ~ X1 + X2 + X1*X2, data = nonlinear_sim)\n\nAgain, we can now take our predicted \\(Y_{i}\\)’s and store them in our dataset to plot against our observed \\(Y_{i}\\)’s.\n\nnonlinear_sim$y_hat &lt;- predict(mod2)\n\n# plotting observed vs. fitted y's using ggplot \nlibrary(ggplot2)\nplot2 &lt;- ggplot(nonlinear_sim, aes(x = Y, y = y_hat)) +\n  geom_point(alpha = 0.5) +  \n  labs(x = \"Observed Y\", y = \"Predicted Y\", title = \"Observed vs Predicted Y\") +\n  theme_minimal()\n\nWhen we do this, we see that our predictions are way off. While our predicted \\(Y's\\) range from 200 to about 800, our observed \\(Y\\)’s range from about 500 to 6000.\n\n\n\n\n\nAlthough there is indeed a systematic relationship between \\(X_1\\) and \\(X_2\\) because this relationship isn’t additive and linear, our model can’t detect it and effectively use \\(X_1\\) and \\(X_2\\) to predict \\(Y\\)."
  },
  {
    "objectID": "posts/2024-10-03/index.html#tree-based-models",
    "href": "posts/2024-10-03/index.html#tree-based-models",
    "title": "A Practical Introduction to Machine Learning in Psychology",
    "section": "Tree Based Models",
    "text": "Tree Based Models"
  },
  {
    "objectID": "posts/2024-10-03/index.html#enter-tree-based-models",
    "href": "posts/2024-10-03/index.html#enter-tree-based-models",
    "title": "A Practical Introduction to Machine Learning in Psychology",
    "section": "Enter tree based models",
    "text": "Enter tree based models"
  },
  {
    "objectID": "posts/2024-10-03/index.html#a-relevant-social-psychology-example-factors-that-predict-sharing-misinformation-on-social-media",
    "href": "posts/2024-10-03/index.html#a-relevant-social-psychology-example-factors-that-predict-sharing-misinformation-on-social-media",
    "title": "A Practical Introduction to Tree-Based Machine Learning Methods",
    "section": "A relevant social psychology example: factors that predict sharing misinformation on social media",
    "text": "A relevant social psychology example: factors that predict sharing misinformation on social media"
  },
  {
    "objectID": "posts/2024-10-03/index.html#another-simulation-tree-based-methods-vs.-linear-regression-for-prediction-under-different-data-generating-processes",
    "href": "posts/2024-10-03/index.html#another-simulation-tree-based-methods-vs.-linear-regression-for-prediction-under-different-data-generating-processes",
    "title": "A Practical Introduction to Tree-Based Machine Learning Methods",
    "section": "Another simulation: tree based methods vs. linear regression for prediction under different data generating processes",
    "text": "Another simulation: tree based methods vs. linear regression for prediction under different data generating processes"
  },
  {
    "objectID": "posts/2024-10-03/index.html#a-simple-simulation-what-happens-to-our-predictions-from-linear-regression-when-linearity-doesnt-hold",
    "href": "posts/2024-10-03/index.html#a-simple-simulation-what-happens-to-our-predictions-from-linear-regression-when-linearity-doesnt-hold",
    "title": "A Practical Introduction to Tree-Based Machine Learning Methods",
    "section": "A simple simulation: What happens to our predictions from linear regression when linearity doesn’t hold?",
    "text": "A simple simulation: What happens to our predictions from linear regression when linearity doesn’t hold?\nSo how does linear regression fair when the linearity assumption is violated? Not well, but you don’t have to just take my word for it. Consider the following simulation. We have two datasets consisting of two different data generating processes. One conforms to the assumed data-generating process of the linear model, the other takes the form of the second equation above. We’ll start with the DGP that conforms to the assumptions of the linear model, before seeing what happens to our predicted vs. observed \\(Y_i\\)’s when our true DGP violates the linearity assumption.\n\n#------------------- Case 1: We meet the linearity assumption------- #\n# First seet our seed since we're simulating with random draws\nset.seed(919)\n# Assumed DGP linear model \nX1 &lt;- rnorm(1000, mean = 10, sd = .5)\nX2 &lt;- runif(1000, min = 0, max = 10)\nY &lt;- .3*X1 + .2*X2 + .7*X1*X2 \nerror &lt;- rnorm(1000, mean=0, sd = .9)\nY &lt;- Y + error\n\n# combine into dataset \nlinear_sim &lt;- as.data.frame(cbind(Y, X1, X2)) \n\n# fit linear model \nmod &lt;- lm(Y ~ X1 + X2 + X1*X2, data = linear_sim)\n\nNow that we’ve simulated our data, and fitted our linear regression model, we can generate our predicted \\(\\hat{Y}_{i}\\) values using our model and store that information in our dataset. Then we can visually examine the accuracy of our model’s predictions by plotting the predicted \\(\\hat{Y_i}\\)’s against the observed \\(Y_i\\)’s. We can do this using like so:\n\nlinear_sim$y_hat &lt;- predict(mod)\n\n# plotting observed vs. fitted y's using ggplot \nlibrary(ggplot2)\nplot &lt;- ggplot(linear_sim, aes(x = Y, y = y_hat)) +\n  geom_point(alpha = 0.5) +  \n  geom_abline(slope = 1, intercept = 0, color = \"red\", linetype = \"dashed\") +  \n  # add reference line,\n  #falling on the 45 degree line \n  # means observed and predicted match (perfect fit)\n  labs(x = \"Observed Y\", y = \"Predicted Y\", title = \"Observed vs Predicted Y\") +\n  theme_minimal()\n\nAs you can see below, although there is a bit of variation (the random error in our data generating process), most observations fall very close to the 45 degree line. Our linear regression model does a very good job of predicting our \\(Y\\).\n\n\n\n\n\nBut what if our true data generating process is highly non-linear and non additive? There’s a systematic relationship between \\(Y\\) and our predictor variables, but the relationship doesn’t take the form assumed by the linear model. Let’s run another simulation and see.\n\n#------------------- Case 2: We FAIL to meet the linearity assumption------- #\n# First seet our seed since we're simulating with random draws\nset.seed(919)\n# Non-linear DGP\nX1 &lt;- rnorm(1000, mean = 10, sd = .5)\nX2 &lt;- runif(1000, min = 0, max = 10)\nY &lt;- (3000*X1)/((X2)^(1/3))*.025\nerror &lt;- rnorm(1000, mean=0, sd = .9)\nY &lt;- Y + error\n\n# combine into dataset \nnonlinear_sim &lt;- as.data.frame(cbind(Y, X1, X2)) \n\n# fit linear model \nmod2 &lt;- lm(Y ~ X1 + X2 + X1*X2, data = nonlinear_sim)\n\nAgain, we can now take our predicted \\(Y_{i}\\)’s and store them in our dataset to plot against our observed \\(Y_{i}\\)’s.\n\nnonlinear_sim$y_hat &lt;- predict(mod2)\n\n# plotting observed vs. fitted y's using ggplot \nlibrary(ggplot2)\nplot2 &lt;- ggplot(nonlinear_sim, aes(x = Y, y = y_hat)) +\n  geom_point(alpha = 0.5) +  \n  labs(x = \"Observed Y\", y = \"Predicted Y\", title = \"Observed vs Predicted Y\") +\n  theme_minimal()\n\nWhen we do this, we see that our predictions are way off. While our predicted \\(Y's\\) range from 200 to about 800, our observed \\(Y\\)’s range from about 500 to 6000.\n\n\n\n\n\nAlthough there is indeed a systematic relationship between \\(X_1\\) and \\(X_2\\) because this relationship isn’t additive and linear, our model can’t detect it and effectively use \\(X_1\\) and \\(X_2\\) to predict \\(Y\\)."
  },
  {
    "objectID": "posts/2024-10-03/index.html#a-relevant-social-psychology-example-simulated-factors-that-predict-sharing-misinformation-on-social-media",
    "href": "posts/2024-10-03/index.html#a-relevant-social-psychology-example-simulated-factors-that-predict-sharing-misinformation-on-social-media",
    "title": "A Practical Introduction to Tree-Based Machine Learning Methods",
    "section": "A relevant social psychology example: (simulated) factors that predict sharing misinformation on social media",
    "text": "A relevant social psychology example: (simulated) factors that predict sharing misinformation on social media"
  }
]